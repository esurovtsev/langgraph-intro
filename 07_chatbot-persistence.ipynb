{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Messages Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MongoDB container\n",
    "\n",
    "@docker-compose.yml\n",
    "\n",
    "```bash\n",
    "docker compose up\n",
    "\n",
    "http://localhost:8081"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining MongoDB checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "mongo_uri = \"mongodb://root:example@localhost:27017/chatbotdb?authSource=admin\"\n",
    "mongo_client = MongoClient(mongo_uri)\n",
    "db = mongo_client.get_database()\n",
    "\n",
    "# Create MongoDB checkpoint \n",
    "from langgraph.checkpoint.mongodb import MongoDBSaver\n",
    "mongo_memory = MongoDBSaver(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining chatbot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, RemoveMessage\n",
    "\n",
    "\n",
    "# OPENAI_API_KEY environment variable must be set\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Defining Schema\n",
    "##################################################################################\n",
    "class SummaryState(MessagesState):\n",
    "    question: str\n",
    "    answer: str\n",
    "    summary: str\n",
    "\n",
    "\n",
    "# System message\n",
    "chatbot_system_message = SystemMessage(content=(\"\"\"\n",
    "You are a helpful and knowledgeable chatbot assistant. \n",
    "Your goal is to provide clear and accurate answers to user questions based on the information they provide. \n",
    "Stay focused, concise, and ensure your responses are relevant to the context of the conversation. \n",
    "If you don’t have enough information, ask for clarification.”\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def chatbot(state: SummaryState) -> SummaryState:\n",
    "    summary = state.get(\"summary\", \"\") # getting summary if it exists\n",
    "\n",
    "    # If there is summary, then we add it\n",
    "    if summary:\n",
    "        # define summary as SystemMessage\n",
    "        summary_message = SystemMessage(content=(f\"\"\"\n",
    "        Summary of Conversation:\n",
    "\n",
    "        {summary}\n",
    "        \"\"\"))\n",
    "\n",
    "        messages_with_summary = [summary_message] + state[\"messages\"]\n",
    "    \n",
    "    else:\n",
    "        messages_with_summary = state[\"messages\"]\n",
    "\n",
    "\n",
    "    question = HumanMessage(content=state.get(\"question\", \"\"))\n",
    "\n",
    "    response = llm.invoke([chatbot_system_message] + messages_with_summary + [question]);\n",
    "\n",
    "    return SummaryState(\n",
    "        messages = [question, response],\n",
    "        question = state.get(\"question\", None),\n",
    "        answer = response.content,\n",
    "        summary = state.get(\"summary\", None)\n",
    "    )\n",
    "\n",
    "\n",
    "def summarize(state: SummaryState) -> SummaryState:\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    # no system message\n",
    "    # the order of components is important\n",
    "\n",
    "    if summary:\n",
    "        summary_message = HumanMessage(content=(f\"\"\"\n",
    "            Expand the summary below by incorporating the above conversation while preserving context, key points, and \n",
    "            user intent. Rework the summary if needed. Ensure that no critical information is lost and that the \n",
    "            conversation can continue naturally without gaps. Keep the summary concise yet informative, removing \n",
    "            unnecessary repetition while maintaining clarity.\n",
    "            \n",
    "            Only return the updated summary. Do not add explanations, section headers, or extra commentary.\n",
    "\n",
    "            Existing summary:\n",
    "\n",
    "            {summary}\n",
    "            \"\"\")\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        summary_message = HumanMessage(content=\"\"\"\n",
    "        Summarize the above conversation while preserving full context, key points, and user intent. Your response \n",
    "        should be concise yet detailed enough to ensure seamless continuation of the discussion. Avoid redundancy, \n",
    "        maintain clarity, and retain all necessary details for future exchanges.\n",
    "\n",
    "        Only return the summarized content. Do not add explanations, section headers, or extra commentary.\n",
    "        \"\"\")\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [summary_message]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    \n",
    "    return SummaryState(\n",
    "        messages = delete_messages,\n",
    "        question = state.get(\"question\", None),\n",
    "        answer = state.get(\"answer\", None),\n",
    "        summary = response.content\n",
    "    )\n",
    "\n",
    "\n",
    "# Edges\n",
    "\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_summarize(state: SummaryState):\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    if len(messages) > 2:\n",
    "        return \"summarize\"\n",
    "    \n",
    "    return END\n",
    "\n",
    "\n",
    "# Graph\n",
    "workflow = StateGraph(SummaryState)\n",
    "workflow.add_node(chatbot)\n",
    "workflow.add_node(summarize)\n",
    "\n",
    "workflow.add_edge(START, \"chatbot\")\n",
    "workflow.add_conditional_edges(\"chatbot\", should_summarize)\n",
    "workflow.add_edge(\"summarize\", END)\n",
    "\n",
    "\n",
    "graph = workflow.compile(checkpointer=mongo_memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = \"1\"\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "graph.invoke(SummaryState(question=\"Hi, I’m working on a Python project, and I’m stuck with handling API responses.\"), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = \"1\"\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "graph.invoke(SummaryState(question=\"Sorry what was my previous question?\"), config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
